---
title: "Análise das viagens de taxi em Nova York"
author: "Rafael"
date: August 02, 2018
output:
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(ggplot2)
library(dplyr)
options(scipen = 999)
library(lubridate)
library(scales)
library(tidyquant)
library(gridExtra)
library(maptools)
library(maps)
library(gganimate)
```


```{python echo=FALSE, include=FALSE}
# Manipulação de dados
import pandas as pd
import numpy as np
from datetime import datetime


# conexão com o banco de dados
import psycopg2
import configparser
conf = configparser.ConfigParser()
conf.read('dwh.cfg')
conn = psycopg2.connect("host={} dbname={} user={} password={} port={}".format(*conf['CLUSTER'].values()))
cur = conn.cursor()
conn.set_session(autocommit=True)


# Visualização de dados
import matplotlib.pyplot as plt
import seaborn as sns
```


## Introdução

O objetivo desse relatório é extrair insights e, ao mesmo tempo, validar os dados que foram inseridos no banco de dados.

Com o intuito de melhorar a qualidade do relatório, foi utilizado uma mistura de R, Python e SQL.

- SQL foi utilizado para extração dos dados do banco de dados
- Python para manipulação dos dados e a transformação dos dados requisitados do banco de dados em um DataFrame
- R foi utilizado na confecção dos gráficos porque, apesar do Python possuir bibliotecas capazes de gerar gráficos, tais como seaborn, matplotlib ou a própria biblioteca de gráficos do Pandas, ainda considero o R mais flexível nesse ponto e, por conta disso, optei pela utilização da biblioteca ggplot2 para geração dos gráficos.
- Para realizar a combinação de R com Python foi utilizado a biblioteca reticulate.
- O código utilizado para geração dos gráficos pode ser ativado ao clicar na caixinha `code`


## Quesitos Mínimos  


### Distância média percorrida em viagens com no máximo dois passageiros

```{python echo=FALSE, include=FALSE}
# Calculando a distância média
cur.execute("SELECT AVG(trip_distance) FROM trips WHERE passenger_count <= 2")
average_distance = cur.fetchone()


# coletando todos os registros relacionados a distância em corridas com até dois passageiros
cur.execute("SELECT trip_distance FROM trips WHERE passenger_count <= 2")
distribution = cur.fetchall()

# transformando o retorno do banco de dados em uma lista
distribution = [i[0] for i in distribution]
```



```{r include=TRUE, echo=FALSE}
# criando um data-frame do R.
distribution <- as.data.frame(py$distribution)

# renomeando a coluna
names(distribution) <- 'data'


distribution %>%
  
  # Com o intuito de ajustar o histograma optei pela utilização de um subset dos dados
  subset(data < quantile(data, 0.975)) %>%
  
  
  # A intenção é criar um histograma, dessa forma, só é necessário uma dimensão.
  ggplot(aes(data)) + 
  
  # Adicionando o tipo de gráfico que vai ser utilizado
  geom_histogram(bins=30, fill="#2c3e50") +
  
  # Adicionando a linha que indica a média
  geom_vline(xintercept = mean(distribution$data), color="red") +
  
  theme_tq() + 
  
  # Adicionando os labels.
  labs(title='Distribuição das distância das corridas',
       subtitle = "Somente corridas com até dois passageiros",
       x='Distância (milhas)',
       y='Número de corridas')
  
```

```{r}
distribution %>%
  summary()
```


Uma das informações que foi solicitada era a distância média em viagens que tinham até dois passageiros. Com o intuito de tornar esse dado mais descritivo, optei pela criação de um histograma com uma linha vertical interceptando a média porque possibilita a visualização de valores que estão muito acimas da média e de valores muito abaixo da média.

Em média, em cada corrida realizada, são percorridas 2.66 milhas.

Um ponto interessante que pode ser visualizado através do histograma que não poderia ser visualizado a partir da média é que os dados relativos a distância possuem uma assimetria a direita e, por conta disso, a média é superior a mediana e a moda. Isso ocorre porque a média é mais sensível a presença de outliers.


Por fim,  Um detalhe que chamou atenção é que entre as corridas com até dois passageiros, havia corridas em que o número de passageiros era igual a zero. Por conta disso, chequei as outras variáveis existentes nessas observações para observar se houve cobrança por essas corridas. Em todas elas houve cobrança e, por conta disso, optei pela manutenção delas no banco de dados.

### Quantidade total arrecadada pelas três maiores empresas de taxi de Nova York

##### Empresas existentes na cidade

```{python}

# extraindo os dados da tabela vendors
cur.execute("SELECT vendor_id, nome, current FROM vendors")
vendors = cur.fetchall()

# Criando uma lista de dicionários que será utilizada para criação de um DataFrame
vendors = [{'id': i[0], 'nome': i[1], 'current': i[2]} for i in vendors]

vendors = pd.DataFrame(vendors)
```

```{r}
# fazendo a transição do Python para o R
vendors <- py$vendors

vendors
```


Como pode ser observado acima, existem quatro empresas de taxi em operação e uma que não está operando.


```{python}
# Query utilizado para adquirir tanto o faturamento total por empresa, como o número
# de corridas realizadas por cada uma delas entre 2009 e 2012.
cur.execute("""
SELECT nome, current, SUM(total_amount), COUNT(*)
FROM trips t
JOIN vendors v
ON t.vendor_id = v.vendor_id
WHERE total_amount IS NOT NULL
GROUP BY 1, 2
ORDER BY 3 DESC
""")
results = cur.fetchall()

# checando se o retorno possui o mesmo número de linhas que a tabela na cloud
assert sum([i[3] for i in results]) == 4000000

# transformando os dados em uma lista de dicionários para facilitar o processo de conversão para um DataFrame
quantidade_arrecadada = [{'vendor': i[0], 'valor_total': i[2], 
                         'numero_de_corridas': i[3]} for i in results]

# transformando em um DataFrame que será utilizado no R.
quantidade_arrecadada = pd.DataFrame(quantidade_arrecadada)
```



```{r fig.width=12, fig.height=5}
quantidade_arrecadada <- as.data.frame(py$quantidade_arrecadada)

# Criação do gráfico
plot1 <- quantidade_arrecadada %>%
  # removendo a empresa que possui a menor quantidade arrecadada
  filter(valor_total != min(valor_total)) %>%
  
  # criando uma coluna com o valor arrecadado em dólares para ser utilizado como label
  mutate(valor_total_text = dollar(valor_total)) %>%
  
  # As colunas serão reorganizadas para os dados serem dispostos em ordem decrescente
  ggplot(aes(reorder(vendor, -valor_total), valor_total)) + 
  
  # criando gráfico de colunas
  geom_col(fill="#C40003") +
  
  geom_smooth(method='lm', se=FALSE) +
  
  # Formatando o gráfico
  theme_tq() + 
  geom_label(aes(label = valor_total_text)) +
  scale_y_continuous(labels = dollar) +
  labs(title='Quantidade arrecadada pelas três maiores empresas', 
       x='empresas de taxi',
       y='total arrecadado')



plot2 <- quantidade_arrecadada %>%
  # removendo a empresa que possui a menor quantidade arrecadada
  filter(valor_total != min(valor_total)) %>%
  
  # As colunas serão reorganizadas para os dados serem dispostos em ordem decrescente
  ggplot(aes(reorder(vendor, -numero_de_corridas), numero_de_corridas)) + 
  
  # criando gráfico de colunas
  geom_col(fill="#2c3e50") +
  
  # formatando os gráficos
  geom_smooth(method='lm', se=FALSE) + 
  geom_label(aes(label = numero_de_corridas)) +
  theme_tq() + 
  labs(
    title='Número de corridas realizadas pelas três maiores empresas', 
    x='empresas de taxi',
    y='número de corridas'
    )


plot <- arrangeGrob(plot1, plot2, ncol=2)
grid.arrange(plot)
```

Entre as quatro empresas que estão em operação, somente três tiveram faturamentos expressivos - mais de um milhão de dólares - entre 2009 e 2012. Também sendo possível observar que existe uma concentração do número de corridas e do faturamento nas empresas **Creative Mobile Technologies, LLC** e **VeriFone Inc**. Para se ter uma noção dessa concentração nas duas primeiras, a segunda empresa arrecadou, aproximadamente, sete vezes o que a terceira arrecadou.


### Distribuição mensal das corridas pagas em dinheiro nos últimos quatro anos


```{python include=FALSE, echo=FALSE}
cur.execute("""
SELECT EXTRACT(year FROM pickup_datetime) AS year,
    EXTRACT(month FROM pickup_datetime) AS month,
      COUNT(*)
FROM trips
WHERE payment_type = 'Cash'
GROUP BY 1, 2
""")
results = cur.fetchall()


# transformando os dados obtidos em um dataframe com o index igual a data.
distribuição = [{'data_mês': datetime.strptime(str(i[0]) + '-' + str(i[1]), '%Y-%m'), 
                 'numero_de_corridas': i[2]} for i in results]
                 
distribuição = pd.DataFrame(distribuição)

distribuição['mês'] = distribuição['data_mês'].dt.month

month_mappers = {1: 'Jan', 2: 'Fev', 3: 'Mar', 4: 'Abr', 5: 'Mai', 6: 'Jun',
          7: 'Jul', 8: 'Ago', 9: 'Set', 10: 'Out', 11: 'Nov', 12: 'Dez'}
          
distribuição['mês_texto'] = distribuição['mês'].map(month_mappers)
```



```{r}
distribuição <- py$distribuição

distribuição %>%
  ggplot(aes(numero_de_corridas)) + geom_histogram(fill="#2c3e50", bins=30) + 
  labs(title='Distribuição das corridas pagas em dinheiro por mês (2009-2012)',
       x='número de corridas', y='número de meses')
```

Um dos pontos que chama a atenção é que a grande maioria dos meses possui entre 50000 e 80000 corridas, enquanto em alguns meses há somente 25000 corridas.

Tendo isso em mente, é importante identificar em quais meses há essa oscilação negativa. Para além disso, também é necessário observar se existe dentro da amostra algum mẽs em que não houve registro de corridas de taxi. 

Uma forma simples de checar a existência de meses em que não houveram corridas é comparar a quantidade de valores que foi utilizado para criação do histograma com a quantidade que deveria ser utilizada (48)


```{r}
distribuição %>%
  
  # Manipulação dos dados
  mutate(month = month(data_mês)) %>%
  group_by(month, mês_texto) %>%
  summarize(count = n()) %>%
  
  # Criação de um gráfico de barras
  ggplot(aes(x=reorder(mês_texto, month), y=count)) + 
  geom_bar(stat='identity', fill="#2c3e50")  +


  # Formatação do gráfico
  theme_tq() + 
  labs(
    title='Presença de registros por mẽs',
    x='Mês',
    y='Quantidade de vezes nos quatro anos'
  )
  
```

Um fato que chama a atenção é que em alguns dos anos contemplados pela amostra, existem dois meses de dezembro e um mês de novembro em que não há registro de nenhuma corrida.

Para se ter uma noção mais profunda de como isso afeta os dados que estão sendo observados, será criado um gráfico compara a quantidade de dias esperada com a quantidade de dias presente no banco de dados.

Com esse intuito, será utilizado o processo abaixo:

      1. Cálculo do total de dias por mẽs nos ultimos quatro anos
      2. Divisão desse valor por quatro
      3. Diferença entre a quantidade de dias esperada para cada mês e a quantidade de dias
      média presente no banco de dados.
      4. O valor resultante da etapa trẽs foi subtraido de 1 para obtenção do percentual de valores
      ausentes por mês nos últimos quatro anos.


```{python}
# Extraindo o número de dias por mês nos quatros anos do banco de dados
# Para tanto foi necessário a utilização de um subquery.
cur.execute("""
SELECT EXTRACT(month FROM t1.date), 
       COUNT(*)
FROM (SELECT pickup_datetime::date AS date, COUNT(*)
     FROM trips
     GROUP BY 1) t1
GROUP BY 1
ORDER BY 1
""")

# Manipulação dos dados com o intuito de criar um dataframe
results = cur.fetchall()
results = [{'mês': key, 'numero_de_dias_com_registros': value} for key, value in results]
results = pd.DataFrame(results).sort_values(by='mês', ascending=True)


results['mês_texto'] = results['mês'].map(month_mappers)
```

```{r}

# vetor com os dias esperados por mês em ordem
dias_por_mes <- c(31,28,31,30,31,30,31,31,30,31,30,31)
by_month_year <- py$results

table_dias <- by_month_year %>%
  
  # dividindo a quantidade de dias total por mês por quatro, para obter a quantidade média por ano
  mutate(numero_medio_por_ano = numero_de_dias_com_registros / 4) %>%
  
  # utilizando o vetor que foi criado na primeira linha para a criação de uma nova coluna no DF.
  mutate(total_dias = dias_por_mes) %>%
  
  # criação de uma coluna com o percentual de datas que não estão presentes no banco de dados.
  mutate(percent_missing_or_zero = 1 - (numero_medio_por_ano / total_dias)) %>%
  
  # mudando a formatação para porcentagem
  mutate(percent_text = percent(percent_missing_or_zero))
  


# criando o gráfico de percentual de dias que não estão presentes por mẽs
# no banco de dados
table_dias %>%
  
  # omitindo valores igual a zero
  subset(percent_missing_or_zero > 0) %>%
  
  # ordenando pelo mês
  ggplot(aes(x = reorder(mês_texto, mês), y=percent_missing_or_zero)) + 
  geom_col(fill="#2c3e50") + 

  
  # formatação do gráfico
  geom_label(aes(label = percent_text)) +
  scale_y_continuous(labels = percent) +
  theme_tq() +
  labs(
    title="Percentual de dias em que não há registros de corridas por mês",
    subtitle = "Meses com uma média igual ao número de dias naquele mês foram omitidos",
    x="Percentual de dias",
    y="Mês"
  )


```

Através da observações do gráfico, pode ser percebido que entre Novembro e Janeiro, existe uma grande quantidade de dias em que não há a presença de registros no nosso banco de dados. A existência desses valores afeta diretamente os resultados do próximo gráfico que diz respeito ao total de gorgetas pagos as empresas nos últimos três meses de 2012.

### Total de gorgetas por dia nos últimos três meses de 2012


```{python}
# Extraindo o valor total das gorgetas obtidas por dia entre outubro de 2012 e dez de 2012.
cur.execute("""
SELECT pickup_datetime::date, SUM(tip_amount)
FROM trips
WHERE EXTRACT(month FROM pickup_datetime) >= 10
AND EXTRACT(year FROM pickup_datetime) = 2012
GROUP BY 1
""")
results = cur.fetchall()


results = [{'date': value[0], 'quantidade_diária': value[1]} for value in results]

# criando um Dataframe ordenado pela data
results = pd.DataFrame(results).sort_values(ascending=True, by='date')

# convertando a coluna `date` para um formato que pode ser interpretado pelo R.
results['date'] = pd.to_datetime(results['date'])


```


```{r}
gorgetas <- py$results

gorgetas %>%
  
  # utilizando a função date para converter para date
  mutate(date = date(date)) %>%
  
  # criação de um gráfico de linhas
  ggplot(aes(x=date, y=quantidade_diária)) + 
  geom_line() +
  
  
  # Formatação do gráfico
  scale_y_continuous(labels = dollar, limits = c(0, 2000)) +
  scale_x_date(date_breaks = '4 days') +

  theme_tq() + 
  labs(
    title = 'Total de gorgetas por dia (Out-Dez) em 2012',
    subtitle = 'Não há registros nos banco de dados a partir de 27-10-2012',
    x='data',
    y='total de gorgetas (US$)'
  )
  

```



Como dito anteriormente, a existência de dias em que não há existẽncia de registros de corridas afeta diretamente o resultado desse gráfico. O último valor presente no banco de dados é referente ao dia de 27-10-2012.

Não da para inferir o motivo da ausência desses valores com os dados disponíveis. A hipótese mais provável é que esses dados estão realmente faltando porque, pelo menos no mês de Outubro, não há uma variação muito grande no total de gorgetas que foram concedidas pelos clientes. Outra possibilidade, menos provável, é que não houveram corridas entre 27 de Outubro e 31 de Dezembro.

Para averiguar o real motivo da não existência desses registros seria necessário extrair os dados diretamente da fonte desses dados.


## Quesitos Extras


### Tempo médio das corridas realizadas no sábado e no domingo

```{python}
# Adotei uma estratégia mais conservadora e optei por extrair os dados em seu formato bruto
# do banco de dados com o intuito de evitar potenciais erros no resultado
# A única modificação foi a adoção de um filtro para extrair somente as corridas que 
# iniciaram no sábado ou no domingo
cur.execute("""
SELECT dropoff_datetime, pickup_datetime
FROM trips
WHERE EXTRACT(dow FROM pickup_datetime) IN (0.0, 6.0)
""")

results = cur.fetchall()

# Aqui faz se necessário passar o formato para o valor ser convertido para uma data
# antes de ser inserido no DataFrame
results = [{'dropoff_datetime': datetime.strftime(i[0],
             format="%Y-%m-%d %H:%M:%S"), 
            'pickup_datetime': datetime.strftime(i[1],
             format="%Y-%m-%d %H:%M:%S")} for i in results]


# Criação do dataframe e conversão para o formato datetime do pandas
results = pd.DataFrame(results)
results['dropoff_datetime'] = pd.to_datetime(results['dropoff_datetime'])
results['pickup_datetime'] = pd.to_datetime(results['pickup_datetime'])


```

- Processo de extração dessas informações;

    1. Para obtenção dos dias da semana, foi utilizada a função dow da Amazon Redshift. Essa função retorna valores de
    0 a 6 em que o 0 é igual a domingo e o 6 é igual a Sábado.
    2. Esses dois números foram utilizados como filtro.
    3. Os valores foram manipulados para criação de um DataFrame que posteriormente foi utilizado para calcular a média
    do tempo médio em cada um desses dais.



```{r}
results <- py$results

results %>%
  # obtendo a diferença em minutos
  mutate(tempo_total = difftime(time1 = dropoff_datetime, time2 = pickup_datetime, units = 'mins')) %>%
  
  # extraindo o dia da semana utilizando a função weekdays
  mutate(weekdays = weekdays(as.Date(pickup_datetime))) %>%
  
  # obtendo o tempo médio
  group_by(weekdays) %>%
  summarize(tempo_médio = mean(tempo_total))
```

A partir dos resultados da tabela, observa-se que o tempo médio nas corridas realizadas é praticamente igual nos dois dias. Em ambos os casos (após a realização da conversão da porção dos segundos), tem-se um tempo médio de, aproximadamente, 8 minutos e 45 segundos.

### Faturamento Acumulado ao longo dos quatro anos - Gráfico Animado

```{python}

# O Query abaixo será utilizado para obter a soma cumulativa por empresa ao longo dos quatro anos
# Para tanto, criou-se uma subquery em conjunto com uma window function
# No redshift, diferentemente de RDBMS tradicionais, é necessário especificar o frame
# a ser utilizado `rows unbounded preceding`

cur.execute("""
SELECT t1.vendor_id,
  t1.amount_by_day,
  t1.date,
  SUM(t1.amount_by_day) OVER (PARTITION BY t1.vendor_id ORDER BY t1.date rows unbounded preceding)
FROM (SELECT vendor_id,
   pickup_datetime::date as date,
   SUM(total_amount) AS amount_by_day
   FROM trips t
   GROUP BY 1, 2) t1
""")

# Criação do DataFrame
cumulative_sum = cur.fetchall()
cumulative_sum = [{'vendor_id': i[0], 'daily_value': float(i[1]),
                  'date': i[2], 'cum_sum': i[3]} for i in cumulative_sum]
cumulative_sum = pd.DataFrame(cumulative_sum)

cumulative_sum['date'] = pd.to_datetime(cumulative_sum['date'])
```


```{r fig.height=12, fig.width=12}
cumulative_sum <- py$cumulative_sum


cumulative_sum <- cumulative_sum %>%
  # criando uma variável cum_sum em dólares
  mutate(cum_sum_text = dollar(cum_sum)) %>%
  # mudando o tipo de variável para date
  mutate(date = date(date)) %>%
  # organizando por data
  arrange(date)

# Criação de um gráfico de linha com um gráfico de pontos
p <- ggplot(data=cumulative_sum, aes(x=date, y=cum_sum, color=vendor_id)) +
  geom_line() + 
  geom_point(size=2) +
  
  # Formatação do gráfico
  theme_tq(14,"Avenir") + 
  labs(title = "Faturamento acumulado até {frame_time}", x = "", y = "Faturamento (US$)") +
  scale_fill_tq() +
  theme(legend.position = 'right') +
  scale_y_continuous(labels = dollar) + 
  
  # Características da animação:
  # As animações serão realizadas com base na variável date
  transition_time(date) +
  shadow_wake(wake_length = 1, exclude_layer = 2) +
  ease_aes('cubic-in-out')
  

animate(p)

anim_save('serie_temporal.gif')

```

Uma das coisas que chamam a atenção nesse gráfico é que o faturamento das empresas `Verifone` e `Creative Mobile Solutions` seguem o mesmo padrão ao longo do tempo nesses quatros anos.

Por outro lado, o faturamento da empresa Dependable Driver Service possui uma inclinação mais tênue quando comparada com seus concorrentes.

Por fim, as corridas realizadas pela empresa Total Solutions são muito raras dentro do nosso banco de dados e, por conta disso, ela só aparece no gráfico em algumas datas muito específicas.


### Análise Geoespacial

Uma análise que não é muito comum, mas que pode trazer insights muito importantes para uma empresa é a utilização de análise geoespacial através da utilização das coordenadas geográficas referentes a eventos específicos.

Nesse sentido, no banco de dados disponível, existem quatro variáveis que estão relacionadas ao local em que a corrida começou e ao local em que ela terminou.

Existem algumas corridas no banco de dados em que essas coordenadas não foram disponibilizadas e, por conta disso, optei por não utilizar essas coordenadas nos próximos gráficos que serão criados.

#### Package Maps

Utilizei o package maps e maptools para conseguir o formato do mapa da cidade de Nova York e suas subregiões: Manhatan, Main, Staten Island e Long Island. 

Foi realizada uma filtragem com base nas coordenadas existentes no banco de dados porque o mapa inteiro abarcar toda a região metropolitana de Nova York.

O formato do mapa que será utilizado para extrair insights pode ser observado abaixo:


```{r}
map_data("state") %>%

    
  # extraindo as coordenadas para criação do polígno que será
  # utilizado no processo de criação de um mapa
  filter(region == 'new york') %>%
  filter(lat > 40.5, lat < 41) %>%
  filter(long > -74.5, long < -73.5) %>%
    
      # criando polígno
  ggplot(aes(x = long, y = lat, group = group, fill=factor(subregion))) +
  
  
  # utilizando branco como cor interna do poligno
  geom_polygon() +
  
  # preto como a cor da linha
  geom_path(linetype=1, color='black') +
  
  theme_tq() +
  scale_fill_tq() + 
  
  labs(
    title = 'Sub-Regiões da cidade de Nova York',
    x='Longitude',
    y='Latitude',
    fill='Subregião'
  ) +
  
  theme(legend.position = 'right')
```

Tenho dois objetivos principais com essa análise. O primeiro objetivo é identificar se existe uma concentração maior em locais específicos tanto para origem, como para destino.

O Segundo objetivo é identificar se dependendo do local em que a corrida inicia ou termina, existe alguma tendência que se reflete no preço da corrida.



```{python}

# Seleção das coordenadas geográficas, da distância em milhas e do valor total pago pela corrida
# no ano de 2010
cur.execute("""
SELECT pickup_latitude, 
    pickup_longitude,
    dropoff_latitude,
    dropoff_longitude,
    trip_distance,
    total_amount
FROM trips
WHERE EXTRACT(year from pickup_datetime) = 2010
""")

geospatial = cur.fetchall()


geospatial = [{'pickup_lat': float(i[0]),'pickup_long': float(i[1]),
               'dropoff_lat': float(i[2]), 'dropoff_long': float(i[3]),
               'trip_distance': float(i[4]), 'total_amount': float(i[5])} for i in geospatial]
geospatial = pd.DataFrame(geospatial)


```


```{r}
geospatial <- py$geospatial

# Variáveis que limitarão o mapa
min_lat <- 40.5
max_lat <- 40.9
min_long <- -74.5
max_long <- -73.5
  
data <-  geospatial %>%
  # retirando distâncias muito grandes
  #filter(trip_distance < quantile(trip_distance, 0.98)) %>%
  
  
  # Limitando as coordenadas
  filter(total_amount  < quantile(total_amount, 0.975)) %>%
  filter(pickup_lat > 40.5, pickup_lat < 41) %>%
  filter(pickup_long > -74.5, pickup_long < -73.5) %>%
  filter(dropoff_lat > 40.5, dropoff_lat < 41) %>%
  filter(dropoff_long > -74.5, dropoff_long < -73.5) %>%
  
  # Retirando locais que possuem destino e 
  subset((dropoff_long != pickup_long) & (dropoff_lat != pickup_lat))
  
  
   # Criação do poligno que demarcará Nova York
  map_data("state") %>%

    
  # extraindo as coordenadas para criação do polígno que será
  # utilizado no processo de criação de um mapa
  
  filter(region == 'new york') %>%
  filter(lat > 40.5, lat < 41) %>%
  filter(long > -74.5, long < -73.5) %>%
    
      # criando polígno
  ggplot(aes(x = long, y = lat, group = group)) +
  
  
  # utilizando branco como cor interna do poligno
  geom_polygon(fill = 'white') +
  
  # preto como a cor da linha
  geom_path(linetype=1, color='black') +
  
  # extraindo as coordenadas do banco de dados
  geom_curve(data=data, 
             aes(x=pickup_long, xend=dropoff_long,
                 y=pickup_lat, yend=dropoff_lat,
                 group=NULL, color=total_amount), 
             alpha=0.01) +
  
  
  # criando um gradient utilizando a variável trip_distance
  scale_color_gradient(low='blue', high='orange', labels=dollar) + 
  
  
  # formatação do gráfico
  theme_tq_dark() +
    
  labs(
    title='Principais destinos das viagens de taxi realizadas em Nova York',
    x='Longitude',
    y='Latitude',
    color="Valor pago (US$)"
  )
  
  theme(legend.position = 'right')

  
```
##### Processo de elaboração do gráfico:
     1. Para elaboração desse gráfico, foram retirados valores de origem e de destino que tinham valores iguais.
     2. Com o intuito de tornar o gráfico mais interpretável foi utilizado um alpha de 0.01.
     3. Foi criado um gráfico de curvas unindo as coordenadas de origem e de destino.


A partir do gráfico acima, algo que chama atenção é que viagens com destino a Main e Long Island são mais caras do que as viagens com destino a Manhatan.

Os principais destinos utilizados pelos passageiros são: Manhatan, o centro da subregião e a costa sudoeste de Long Island e o sul da subregião de Main.

Viagens para outros locais ocorrem, mas não foram refletidas por conta da utilização de um alpha muito pequeno.



## Considerações Finais

A realização dessa análise possibilitou a obtenção de alguns insights.

1. É necessário realizar uma checagem nos dados brutos para entender porque existe uma grande quantidade de dias em que não há registro de corridas no banco de dados.
2. As duas empresas que mais faturam são, respectivamente, **Creative Mobile Technologies, LLC** e **VeriFone Inc**.
3. A criação do gráfico geoespacial possibilitou a identificação dos locais em que se concentram as origens e os destinos dos passageiros. Algo que poderia ser investigado futuramente é a relação existente entre o tempo com o espaço através da criação de um gráfico que leve em conta o horário em que a corrida começa e o local em que ela começa.


